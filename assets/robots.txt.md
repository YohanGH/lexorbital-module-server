# robots.txt - Guide d'utilisation

Ce fichier `robots.txt` contr√¥le l'acc√®s des robots d'indexation (moteurs de recherche) √† votre site.

---

## üìç Emplacement

**Dans le repo** : `assets/robots.txt`  
**Sur le serveur** : `/var/www/assets/robots.txt`

---

## üîß Configuration Nginx

Le fichier est servi via Nginx avec cette configuration :

```nginx
location = /robots.txt {
    alias /var/www/assets/robots.txt;
    access_log off;        # GDPR: pas de logs pour robots.txt
    expires 7d;            # Cache 7 jours
}
```

**Acc√®s** : `https://example.com/robots.txt`

---

## üìù Personnalisation

### 1. Autoriser/Interdire des chemins

**Autoriser tout** :
```
User-agent: *
Allow: /
```

**Interdire des chemins sp√©cifiques** :
```
Disallow: /api/
Disallow: /admin/
Disallow: /private/
```

### 2. Ajouter votre sitemap

D√©commentez et modifiez cette ligne :
```
Sitemap: https://example.com/sitemap.xml
```

### 3. R√®gles sp√©cifiques par moteur

Le fichier inclut d√©j√† des r√®gles pour Google et Bing. Vous pouvez ajouter d'autres moteurs :

```
User-agent: DuckDuckBot
Allow: /
Disallow: /api/
```

---

## üéØ Exemples de configurations

### Configuration permissive (site public)
```
User-agent: *
Allow: /
Disallow: /api/
Disallow: /admin/
Sitemap: https://example.com/sitemap.xml
```

### Configuration restrictive (site priv√©)
```
User-agent: *
Disallow: /
```

### Configuration mixte (certaines pages publiques)
```
User-agent: *
Allow: /public/
Allow: /blog/
Disallow: /
Sitemap: https://example.com/sitemap.xml
```

---

## ‚úÖ Bonnes pratiques

### 1. RGPD et confidentialit√©
- ‚úÖ Ne pas exposer de donn√©es personnelles dans les pages crawlables
- ‚úÖ Interdire l'indexation des zones priv√©es (`/admin/`, `/api/`)
- ‚úÖ V√©rifier que les pages publiques ne contiennent pas de PII

### 2. SEO
- ‚úÖ Inclure votre sitemap (`Sitemap: https://example.com/sitemap.xml`)
- ‚úÖ Autoriser l'indexation des pages importantes
- ‚úÖ Interdire les pages sans valeur SEO (erreurs, admin)

### 3. Performance
- ‚úÖ Interdire les chemins qui g√©n√®rent beaucoup de trafic inutile
- ‚úÖ Utiliser `Crawl-delay` si n√©cessaire (attention : pas tous les moteurs le respectent)

---

## üß™ Tests

### V√©rifier que le fichier est accessible

```bash
curl https://example.com/robots.txt
```

**R√©sultat attendu** : Le contenu du fichier `robots.txt`

### Tester avec Google Search Console

1. Allez sur [Google Search Console](https://search.google.com/search-console)
2. Utilisez l'outil "Tester le fichier robots.txt"
3. V√©rifiez que les r√®gles sont correctement interpr√©t√©es

### Valider la syntaxe

Utilisez un validateur en ligne :
- [Google Search Console Robots.txt Tester](https://www.google.com/webmasters/tools/robots-txt)
- [Robots.txt Checker](https://www.seoptimer.com/robots-txt-checker)

---

## üìö Ressources

### Documentation officielle
- [Google - robots.txt](https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt)
- [RFC 9309 - robots.txt](https://www.rfc-editor.org/rfc/rfc9309.html)

### Outils
- [Google Search Console](https://search.google.com/search-console)
- [Bing Webmaster Tools](https://www.bing.com/webmasters)

---

## ‚ö†Ô∏è Notes importantes

1. **Le fichier est public** : N'importe qui peut voir votre `robots.txt` en visitant `https://example.com/robots.txt`

2. **Pas de s√©curit√©** : `robots.txt` est une suggestion, pas une protection. Les robots malveillants peuvent l'ignorer.

3. **Mise √† jour** : Apr√®s modification, attendez quelques jours pour que les moteurs de recherche prennent en compte les changements.

4. **Cache** : Les moteurs de recherche mettent en cache le fichier. Les changements ne sont pas imm√©diats.

---

**Derni√®re mise √† jour** : 2025-11-30  
**Version** : 1.0.0

